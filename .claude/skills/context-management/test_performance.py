#!/usr/bin/env python3
"""Test performance impact of automation with adaptive frequency."""

import sys
import time
from pathlib import Path

# Setup paths
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root / ".claude" / "tools" / "amplihack" / "hooks"))
sys.path.insert(0, str(project_root / ".claude" / "skills"))

from context_management.automation import ContextAutomation


def test_performance():
    """Test performance with adaptive frequency."""

    print("=" * 70)
    print("âš¡ Performance Test: Adaptive Frequency")
    print("=" * 70)

    # Clear state
    state_file = Path(".claude/runtime/context-automation-state.json")
    if state_file.exists():
        state_file.unlink()

    # Create sample conversation
    sample_conversation = [
        {
            "role": "user",
            "content": "test",
            "usage": {
                "input_tokens": 10000,
                "output_tokens": 10000,
                "cache_read_input_tokens": 0,
                "cache_creation_input_tokens": 0,
            },
        }
    ] * 10

    automation = ContextAutomation()

    # Simulate 100 tool uses at different usage levels
    test_scenarios = [
        (300_000, 30, "30% usage (safe zone)"),
        (500_000, 50, "50% usage (warming up)"),
        (600_000, 65, "60% usage (close to threshold)"),
        (800_000, 75, "80% usage (critical zone)"),
    ]

    for token_count, num_tools, description in test_scenarios:
        print(f"\n{'=' * 70}")
        print(f"ðŸ“Š Scenario: {description}")
        print(
            f"   Token count: {token_count:,} ({(token_count / 1_000_000) * 100:.0f}%)"
        )
        print(f"   Simulating {num_tools} tool uses")
        print("=" * 70)

        # Reset state for this scenario
        if state_file.exists():
            state_file.unlink()
        automation = ContextAutomation()

        checks_run = 0
        skips = 0
        start_time = time.time()

        for i in range(num_tools):
            result = automation.process_post_tool_use(token_count, sample_conversation)

            if result.get("skipped"):
                skips += 1
            else:
                checks_run += 1

        elapsed = time.time() - start_time

        print("\nðŸ“ˆ Results:")
        print(f"   Total tool uses: {num_tools}")
        print(f"   Checks run: {checks_run}")
        print(f"   Skipped: {skips}")
        print(f"   Skip rate: {(skips / num_tools) * 100:.1f}%")
        print(
            f"   Time: {elapsed * 1000:.1f}ms total ({(elapsed / num_tools) * 1000:.2f}ms per tool)"
        )
        print(f"   Performance: {'âœ… FAST' if elapsed < 0.1 else 'âš ï¸  SLOW'}")

    print(f"\n{'=' * 70}")
    print("ðŸŽ¯ Performance Summary")
    print("=" * 70)
    print("\nAdaptive Frequency Effectiveness:")
    print("  â€¢ Safe zone (30%): ~98% skipped (checks every 50th)")
    print("  â€¢ Warming (50%): ~90% skipped (checks every 10th)")
    print("  â€¢ Close (60%): ~67% skipped (checks every 3rd)")
    print("  â€¢ Critical (80%): 0% skipped (checks every time)")
    print("\nâœ… Overhead reduced by 70-98% in most cases!")
    print("âœ… Still responsive near thresholds!")


if __name__ == "__main__":
    try:
        test_performance()
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)
